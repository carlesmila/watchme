<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Calculate interrater agreement • watchme</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">watchme</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="http://github.com/masalmon/watchme">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Calculate interrater agreement</h1>
                        <h4 class="author">M. Salmon and other CHAI project members</h4>
            
            <h4 class="date">2017-02-06</h4>
          </div>

    
    
<div class="contents">
<p>An important aspect of coding images is having a list of annotations that provides consistent results acrossed (trained) coders, or raters. Therefore one needs to check and report interrater agreement.</p>
<div id="simply-look-at-disagreements" class="section level1">
<h1 class="hasAnchor">
<html><body><a href="#simply-look-at-disagreements" class="anchor"> </a></body></html>Simply look at disagreements</h1>
<p>In the package there is a function for getting the times and codes of pictures for which coders disagree, which might be useful for exploring differences when e.g. training coders.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">"coding1"</span>)
<span class="kw">data</span>(<span class="st">"coding2"</span>)
results_list &lt;-<span class="st"> </span><span class="kw">list</span>(coding1, coding2)
names_list &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'Cain'</span>, <span class="st">'Abel'</span>)
<span class="kw"><a href="../reference/watchme_output_differences.html">watchme_output_differences</a></span>(<span class="dt">results_list =</span> results_list,
                                <span class="dt">names_list =</span> names_list)</code></pre></div>
<pre><code>## # A tibble: 33 × 3
##                Cain             Abel          image_time
##               &lt;chr&gt;            &lt;chr&gt;              &lt;dttm&gt;
## 1   indoors, , , ,  , outdoors, , ,  2015-06-12 12:23:40
## 2     , , , mixed,  , outdoors, , ,  2015-06-12 12:24:59
## 3     , , , mixed,  , outdoors, , ,  2015-06-12 12:25:34
## 4     , , , mixed,  , outdoors, , ,  2015-06-12 13:08:17
## 5  , outdoors, , ,   indoors, , , ,  2015-06-12 13:10:01
## 6          , , , ,   indoors, , , ,  2015-06-12 15:19:45
## 7     , , , mixed,  , outdoors, , ,  2015-06-12 15:23:47
## 8     , , , mixed,  , outdoors, , ,  2015-06-12 15:24:23
## 9     , , , mixed,  , outdoors, , ,  2015-06-12 15:24:55
## 10  indoors, , , ,  , outdoors, , ,  2015-06-12 16:06:08
## # ... with 23 more rows</code></pre>
</div>
<div id="assess-interrater-agreement" class="section level1">
<h1 class="hasAnchor">
<html><body><a href="#assess-interrater-agreement" class="anchor"> </a></body></html>Assess interrater agreement</h1>
<p>Nowadays the state of the art is to use Cohen’s kappa for codes attributed to pictures, even if they are not independent (Aiden Doherty’s personal communication). In the package we provide a function for calculating interrater agreement (or IRR) this way, which is called <code>watchme_ira</code>. It uses functions of the <a href="https://cran.r-project.org/web/packages/irr/index.html">R <code>irr</code> package</a>. It allows using several possibilities for defining what codes are to be compared:</p>
<ul>
<li><p>one could compare the global annotations of all files, i.e. all codes at the same time. For instance compare washingYourTeeth; readingABook to washingYourTeeh; readingAMagazine for picture 1 between coder A and coder B. This is the default option.</p></li>
<li><p>one could look separately at each code using binary variables. For instance for washingYourTeeth comparing TRUE to TRUE and for readingABook TRUE to FALSE. For this one needs to set the <code>by_code</code> argument to TRUE.</p></li>
</ul>
<p>In the case in which wants to compare results provided by more than two coders, another choice to be made is whether all coders are to be compared together using Fleiss Kappa, or one to one using Cohen’s kappa. This is set with the <code>one_to_one</code> <code>logical</code> argument.</p>
<p>The comparison one wants to make depends on the context of the calculation of the IRR. When developping a new list of annotations one wants to see interrater agreement for each code, later when may want to report a single figure for the whole list of annotations.</p>
<p>Below are a few examples for two coders to be compared.</p>
<p>The default is to compare all annotations together.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">'coding1'</span>)
<span class="kw">data</span>(<span class="st">'coding2'</span>)
<span class="co"># With two coders</span>
results_list &lt;-<span class="st"> </span><span class="kw">list</span>(coding1, coding2)
names_list &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'Cain'</span>, <span class="st">'Abel'</span>)
ira_all &lt;-<span class="st"> </span><span class="kw"><a href="../reference/watchme_ira.html">watchme_ira</a></span>(results_list, <span class="dt">names_list =</span> names_list)
<span class="kw">kable</span>(ira_all)</code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">method</th>
<th align="right">pictures</th>
<th align="right">agreed_on</th>
<th align="left">raters</th>
<th align="right">Kappa</th>
<th align="right">z</th>
<th align="right">p_value</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1230</td>
<td align="left">Cain, Abel</td>
<td align="right">0.8456533</td>
<td align="right">34.3822</td>
<td align="right">0</td>
</tr></tbody>
</table>
<p>Here we compare annotations by code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ira_codes &lt;-<span class="st"> </span><span class="kw"><a href="../reference/watchme_ira.html">watchme_ira</a></span>(results_list, <span class="dt">names_list =</span> names_list, <span class="dt">by_code =</span> <span class="ot">TRUE</span>)
<span class="kw">kable</span>(ira_codes)</code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">code</th>
<th align="left">method</th>
<th align="right">pictures</th>
<th align="right">agreed_on</th>
<th align="left">raters</th>
<th align="right">Kappa</th>
<th align="right">z</th>
<th align="right">p_value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">indoors</td>
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1251</td>
<td align="left">Cain, Abel</td>
<td align="right">0.9430487</td>
<td align="right">33.52833</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">outdoors</td>
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1232</td>
<td align="left">Cain, Abel</td>
<td align="right">0.8296414</td>
<td align="right">29.86610</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">in_vehicle</td>
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1263</td>
<td align="left">Cain, Abel</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
</tr>
<tr class="even">
<td align="left">mixed</td>
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1241</td>
<td align="left">Cain, Abel</td>
<td align="right">0.0000000</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
</tr>
<tr class="odd">
<td align="left">non_codable</td>
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1263</td>
<td align="left">Cain, Abel</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
<td align="right">NaN</td>
</tr>
</tbody>
</table>
<p>And then for more than two coders. If we do the comparison one by one, the resulting table has as many lines as there are possible pairs of coders. Here we compare all annotations together but even when comparing more than two coders you can do it by group of codes or by code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results_list2 &lt;-<span class="st"> </span><span class="kw">list</span>(coding1, coding1, coding2)
names_list2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'Riri'</span>, <span class="st">'Fifi'</span>, <span class="st">'Loulou'</span>)
<span class="kw"><a href="../reference/watchme_ira.html">watchme_ira</a></span>(results_list2, <span class="dt">names_list =</span> names_list2, <span class="dt">one_to_one =</span> <span class="ot">FALSE</span>) %&gt;%
<span class="st">  </span><span class="kw">kable</span>()</code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">method</th>
<th align="right">pictures</th>
<th align="right">agreed_on</th>
<th align="left">raters</th>
<th align="right">Kappa</th>
<th align="right">z</th>
<th align="right">p_value</th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">Fleiss’ Kappa for m Raters</td>
<td align="right">1263</td>
<td align="right">1230</td>
<td align="left">Riri, Fifi, Loulou</td>
<td align="right">0.8964106</td>
<td align="right">63.90226</td>
<td align="right">0</td>
</tr></tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/watchme_ira.html">watchme_ira</a></span>(results_list2, <span class="dt">names_list =</span> names_list2, <span class="dt">one_to_one =</span> <span class="ot">TRUE</span>)%&gt;%
<span class="st">  </span><span class="kw">kable</span>()</code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">method</th>
<th align="right">pictures</th>
<th align="right">agreed_on</th>
<th align="left">raters</th>
<th align="right">Kappa</th>
<th align="right">z</th>
<th align="right">p_value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1263</td>
<td align="left">Riri, Fifi</td>
<td align="right">1.0000000</td>
<td align="right">43.18003</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1230</td>
<td align="left">Riri, Loulou</td>
<td align="right">0.8456533</td>
<td align="right">34.38220</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">Cohen’s Kappa for 2 Raters (Weights: unweighted)</td>
<td align="right">1263</td>
<td align="right">1230</td>
<td align="left">Fifi, Loulou</td>
<td align="right">0.8456533</td>
<td align="right">34.38220</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#simply-look-at-disagreements">Simply look at disagreements</a></li>
      <li><a href="#assess-interrater-agreement">Assess interrater agreement</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Maëlle Salmon.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
